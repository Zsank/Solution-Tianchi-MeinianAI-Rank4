# 阿里云美年大健康AI算法大赛Rank4

### 队伍：SYSU幼儿园
初赛：A榜Rank Top5/3152， B榜Rank4/3152

复赛：A榜Rank Top5/150，B榜Rank 4/150

> 第一次参加比赛，和小伙伴一起边走边学，其实学到的东西很多。
> 从成绩来看其实我们的模型是很稳定的，我们用的是单GBDT模型，详细情况会在后面展开。

### 比赛介绍
比赛的数据类型涉及数值、符号和文本，需要我们预测五个高血压相关指标，分别为收缩压、舒张压、甘油三酯、高密度脂蛋白胆固醇、低密度脂蛋白胆固醇 。
评分方法以及数据下载，传送门-->[比赛链接](https://tianchi.aliyun.com/competition/information.htm?spm=5176.100067.5678.2.57bb342dl2OdIC&raceId=231654)

### 运行说明
1. 从官网**下载训练**数据并存放到data目录下：
    - meinian_round1_data_part1_20180408.txt
    - meinian_round1_data_part2_20180408.txt
    - meinian_round1_train_20180408.csv
2. 执行顺序：
    - 在`scr`文件下执行`main.py`
    > Noted：NLP部分用的是第二个模型`model_v2_double_cnn`，不包含在`main.py`里面，需要自己去train。

### 模型思路
可以优先参考：[天池数据比赛top5经验分享](https://zhuanlan.zhihu.com/p/38977718)
- 预处理（难点）
    - 文本与数值都有的字段（eg. pd.Series(["3.24","+","阴性","3.2.2"])，不敢批量替换文本，因为不能保证相同文本在各个字段中的分布一致。其次是很多类型的伪文本需要转换。\
    > 针对这个情况：我们通过EDA找到并重点关注——（a）target指标异常人群经常检查的字段；（b）缺失率最低的字段；（c）含高度相关关键词的文本字段。对于这接近40个字段肉眼观察分布并做预处理。
    - 高缺失率
    > （个人想法，欢迎指教）GBDT模型只关注排序，缺失算是一种特殊的数据类型，它的风险跟单一值是一样。也就是说，假如缺失值高于某个阈值就要删除字段的话，那么单一值高于这个阈值的字段也应该一并删除。这种现象可以理解为数据的丰富性集中在少量数据上，这会导致树的分裂结点在一个小区间上波动。分裂后会出现一个只包含很少sample的分支，这个分支的某个sample假如出现数据偏移，造成的负面会影响很大。这是我理解的缺失值对GBDT模型产生风险的过程，实际上通过`min_child_weight` 控制分裂后的sample数就可以抑制这个影响。所以我们不在预处理的时候drop掉这些字段，而在模型后期通过重要性排名和调参来决定这些字段是否入模。
    
- 文本处理
**初赛**用的是词向量 + CNN + 提取全连接层 = 作为LightGBM的新特征
    - 针对约40个高度相关的文本变量建模
    - 剔除停留词等无关词语，重点关注主谓宾
    - 针对长短文本以及主谓宾结构选取不同size的filter，最后concat到一起
    - 词向量长度d(d=64)，长文本字段采用3*d，4*d，5*d这三种卷积层，短文本子段采用1*d，2*d，3*d这三种卷积层，在全连接层的时候进行concat
    